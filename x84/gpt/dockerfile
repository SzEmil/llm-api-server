FROM nvidia/cuda:12.9.0-runtime-ubuntu22.04

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    TOKENIZERS_PARALLELISM=false \
    HF_HOME=/models/.cache \
    TRANSFORMERS_CACHE=/models/.cache \
    PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# 1. System + Python
RUN apt-get update && apt-get install -y python3 python3-pip git && rm -rf /var/lib/apt/lists/*
RUN python3 -m pip install --no-cache-dir --upgrade pip protobuf

# 2. PyTorch (CUDA 12.9)
RUN pip install --no-cache-dir torch --index-url https://download.pytorch.org/whl/cu129

# 3. MXFP4 obsługa — triton + triton_kernels + transformers z GitHuba
RUN pip install --no-cache-dir triton==3.4 \
    git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels \
    git+https://github.com/huggingface/transformers.git

RUN pip install kernels

# 4. Reszta wymagań
WORKDIR /app
COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r /app/requirements.txt --extra-index-url https://download.pytorch.org/whl/cu129

# 5. Kod aplikacji
COPY main.py /app/main.py

EXPOSE 8000
CMD ["uvicorn","main:app","--host","0.0.0.0","--port","8000","--workers","1","--timeout-keep-alive","75"]

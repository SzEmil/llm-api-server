services:
  llm:
    image: szemil99/llm-app:latest
    container_name: llm-api
    ports: ["8000:8000"]
    env_file:
      - .env
    volumes:
      - models:/models
    environment:
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                - gpu
    restart: unless-stopped

volumes:
  models: {}